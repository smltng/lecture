0.  How much time did you spend on this pre-class exercise, and when?
    I spent 3 hours on this pre-class assignment, unfortunately the night before due to some bad planning and interviews interspersed...

1.  What are one or two points that you found least clear in the
    9/22 slide decks (including the narration)?
    The mutex implementation was a little unclear. 

2.  The pthread_mc.c file in the demo subdirectory runs a Monte Carlo
    simulation to estimate the expected value of a uniform random
    variable.  The "-p" option sets the number of processors used,
    while "-b" sets the number of trials between synchronizations.

    a) Write a model for the run time for this simulation code in
<<<<<<< HEAD
       terms of the number of trials (N), number of processors (p),
       time per trial (t_trial), and time to update the global
       counters in the critical section (t_update).
       
    I realized I did this wrong earlier, because this needs Amdahl's Law.
=======
       terms of the number of trials (N), batch size (B),
       number of processors (p), time per trial (t_trial), 
       and time to update the global counters in the critical 
       section (t_update).
>>>>>>> d04643d8830aaf76b3b2a7372f3cf7f454198a78

    S(n) = 1/(B+1/n(1-B))
    
    From Wikipedia: B is the fraction of the algorithm that is strictly serial

    Strictly serial portion is t_update (time it takes to update the global counters in the critical section)

    Total portion is t_trial * N + t_update, so B = t_update/(t_trial * N + t_update)

    S(n) = 1/(t_update/(t_trial*N+t_update)+1/n(1-(t_update/(t_trial*N+t_update))))
    
    b) Run the code with a few different parameter values in order
       to estimate N, t_trial, and t_update for this code on
       a totient compute node.
       I couldn't get this working on a totient compute node, unfortunately, but I did test it on my local machine

    c) Based on your model, suggest a strategy for choosing the batch
       size.  How might you generalize this strategy to automatically
       choose batch sizes for different types of computational
       experiments?

       Choose a batch size such that the amount of time it takes for the slowest processor to finish doesn't cause the rest of the processors to have to wait for a longer time than if the code was not parallelized.

    
3.  In the workq subdirectory of this directory, there is a basic work
    queue implementation.  Following the strategy outlined in the
    slides, add synchronization calls in the locations marked TODO.
    You should run the code to make sure it behaves as expected!
